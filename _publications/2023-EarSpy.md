---
title: "I Can Hear You Without a Microphone: Live Speech Eavesdropping From Earphone Motion Sensors"
collection: publications
permalink: /publication/2023-EarSpy
excerpt: "<img src='/images/teaserEarSpy.png'>"
date: 2023-06-16
venue: 'IEEE INFOCOM'
paperurl: 'http://yetongcao.github.io/files/EarSpy.pdf'
citation: #'Yetong Cao, Qian Zhang, Fan Li, Song Yang, Yu Wang. 2020. &quot;EarAce: Empowering Versatile Acoustic Sensing via Earable Active Noise Cancellation Platform.&quot; <i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</i>. 7(2), 1-23.'
---
**Yetong Cao**, Fan Li*, Huijie Chen, Xiaochen Liu, Chunhui Duan, Yu Wang. "I Can Hear You Without a Microphone: Live Speech Eavesdropping From Earphone Motion Sensors". _IEEE Conference on Computer Communications_, 2023, pp. 1-10.

### Abstract:
Recent literature advances motion sensors mounted on smartphones and AR/VR headsets to speech eavesdropping due to their sensitivity to subtle vibrations. The popularity of motion sensors in earphones has fueled a rise in their sampling rate, which enables various enhanced features. This paper investigates a new threat of eavesdropping via motion sensors of earphones by developing EarSpy, which builds on our observation that the earphone’s accelerometer can capture bone conduction vibrations (BCVs) and ear canal dynamic motions (ECDMs) associated with speaking; they enable EarSpy to derive unique information about the wearer’s speech. Leveraging a study on the motion sensor measurements captured from earphones, EarSpy gains abilities to disentangle the wearer’s live speech from interference caused by body motions and vibrations generated when the earphone’s speaker plays audio. To enable user-independent attacks, EarSpy involves novel efforts, including a trajectory instability reduction method to calibrate the waveform of ECDMs and a data augmentation method to enrich the diversity of BCVs. Moreover, EarSpy explores effective representations from BCVs and ECDMs, and develops a convolutional neural model with Connectionist Temporal Classification (CTC) to realize accurate speech recognition. Extensive experiments involving 14 participants demonstrate that EarSpy reaches a promising recognition for the wearer’s speech.

[<ins>Download PDF</ins>](../files/EarSpy.pdf) 

[<ins>Download slides</ins>](../files/EarSpy.pptx)

